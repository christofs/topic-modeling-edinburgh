<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<!-- CUSTOMIZE THIS! -->
<title>Topic Modeling Workshop Edinburgh</title>
<meta name="author" content="Christof Schöch">
<!-- END -->
<meta name="description" content="Slides">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
<link rel="stylesheet" href="css/reveal.css">
<link rel="stylesheet" href="css/theme/simple.css" id="theme">
<!-- Code syntax highlighting -->
<link rel="stylesheet" href="lib/css/zenburn.css">
<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
<!--[if lt IE 9]>
<script src="lib/js/html5shiv.js"></script>
<![endif]-->
</head>

<body>
<div class="reveal">
<!-- THIS IS WHERE THE CONTENT GOES! -->
<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">

<section data-markdown>
<script type="text/template">
# Topic Modeling <br/>Workshop
<hr/>
<br/>
<br/>
<p><b><a href="http://www.digital.hss.ed.ac.uk/blackbox/">Beyond the Black Box</a></b><br/>Edinburgh, January 2017</p>
<br/>
<hr/>
<p>Christof Schöch<br/>(CLiGS, University of Würzburg, Germany)</p>
<p><img height="50" data-src="img/basics/UWUE.jpg"></img>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img height="100" data-src="img/basics/CLiGS.jpg"></img>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img height="50" data-src="img/basics/BMBF.jpg"></img></p>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Overview
1. What is Topic Modeling?  <!-- .element: class="fragment" data-fragment-index="1" -->
    * How does a topic model look like?
    * How do we arrive at such a model?
2. Interpreting Topic Models <!-- .element: class="fragment" data-fragment-index="2" -->
    * Example analysis
    * Further visualizations
3. Doing Topic Modeling (hands-on) <!-- .element: class="fragment" data-fragment-index="3" -->
    * Telling MALLET what to do
    * Practice time!
</script>
</section>

<section>

<section data-markdown>
<script type="text/template">
# 1. What is Topic Modeling?
</script>
</section>

<section data-markdown>
<script type="text/template">
# (a) Getting started
</script>
</section>


<section data-markdown>
<script type="text/template">
## Topic Modeling: basic idea
* Works on the basis of (large) collections of documents <!-- .element: class="fragment" data-fragment-index="1" -->
* Each document is understood as a mixture of topics <!-- .element: class="fragment" data-fragment-index="2" -->
* The purpose is to discover thematic trends and patterns <!-- .element: class="fragment" data-fragment-index="3" -->
* Discovered through generative probabilistic modeling <!-- .element: class="fragment" data-fragment-index="4" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
## Usage scenarios
* Information Retrieval: Search not for individual terms, but themes / semantic fields <!-- .element: class="fragment" data-fragment-index="1" -->
* Recommender Systems: Recommend similar journal articles etc. to users <!-- .element: class="fragment" data-fragment-index="2" -->
* Exploration of text collections: what is an email or newspaper corpus about? <!-- .element: class="fragment" data-fragment-index="3" -->
* Research questions from literary studies, cultural studies, history: topics across authors, genres, time periods <!-- .element: class="fragment" data-fragment-index="4" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## Explorative Visualization
<p><a href="http://signsat40.signsjournal.org/topic-model/#/model/grid"><img height="500" data-src="img/signs-at-forty.png"></img></a></p>
<p>Signs at 40</p>
</script>
</section>


<section data-markdown>
<script type="text/template">
## Existing Studies
* Cameron Blevins: "Topic Modeling Martha Ballard's Diary" (2010): diary <!-- .element: class="fragment" data-fragment-index="1" -->
* Ted Underwood und Andrew Goldstone (2012): "What can topic models of PMLA teach us...": history of a discipline <!-- .element: class="fragment" data-fragment-index="2" -->
* Lisa Rhody, "Topic Modeling and Figurative Language" (2012): Poesie <!-- .element: class="fragment" data-fragment-index="3" -->
* Matthew Jockers, Macroanalysis (2013): novel, nationality, gender <!-- .element: class="fragment" data-fragment-index="4" -->
* Ben Schmidt: "Typical TV episodes" (2014): TV shows; temporal development <!-- .element: class="fragment" data-fragment-index="5" -->
* Christof Schöch, "Topic Modeling Genre" (2017): drama, subgenres <!-- .element: class="fragment" data-fragment-index="6" -->
</script>
</section>



<section data-markdown>
<script type="text/template">
# (b) How does a topic model look like?
</script>
</section>

<section data-markdown>
<script type="text/template">
## Words, Topics, Documents
<p><img height="500" data-src="img/tm_blei.png"></img></p>
<p>(David Blei, "Probabilistic Topic Models", 2012)</p>
</script>
</section>

<section data-markdown>
<script type="text/template">
## On a practical level
* A topic is a group of words with some (semantic) relation (e.g., common theme) <!-- .element: class="fragment" data-fragment-index="1" -->
* Each topic is made up of words of varying importance and relevance to the topic <!-- .element: class="fragment" data-fragment-index="2" -->
* Each document is made up of several topics in various proportions <!-- .element: class="fragment" data-fragment-index="3" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## On a technical level
* A topic model is an abstract representation of all topics and documents in a collection <!-- .element: class="fragment" data-fragment-index="1" -->
* A topic is a probability distribution over words <!-- .element: class="fragment" data-fragment-index="3" -->
* A document is a probability distribution over topics <!-- .element: class="fragment" data-fragment-index="4" -->
* The Dirichlet distribution (in LDA) describes the topic mixture distribution of the model <!-- .element: class="fragment" data-fragment-index="5" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## Words in topic distribution
<p><img height="400" data-src="img/word-scores-in-topics.png"></img></p>
<p>(Each word has a score in each topic; here ordered by topic/rank)
</script>
</section>

<section data-markdown>
<script type="text/template">
## Topics in document distribution
<p><img height="400" data-src="img/docs-and-topics-in-rank.png"></img></p>
<p>(Each topic has a score in each document; ordered by document)
</script>
</section>

<section data-markdown>
<script type="text/template">
## Dirichlet distributions
<p><a href="img/dirichlet_plot.png"><img height="500" data-src="img/dirichlet_plot.png"></img></a></p>
<p>(Describe the topic mixture distributions of the model)<br/>(Here several possible distributions with three words/topics)</p>
</script>
</section>

<!--
<section data-markdown>
<script type="text/template">
## Dirichlet distributions for both
<p><img height="400" data-src="img/two-dirichlet-distributions.png"></img></p>
<p>(Describe the topic and word mixture distributions of the model)<br/>(Here two possible distributions with three words/topics)</p>
</script>
</section>
-->


<section data-markdown>
<script type="text/template">
## (c) How do we arrive at a topic model?
</script>
</section>

<section data-markdown>
<script type="text/template">
## Some relevant ideas
* The most widespread implementation uses 'Latent Dirichlet Allocation'
* Follows the "bag-of-words"-model: word order is irrelevant <!-- .element: class="fragment" data-fragment-index="1" -->
* No semantic knowledge / dictionary / WordNet etc. is used; language-independent <!-- .element: class="fragment" data-fragment-index="2" -->
* Based on distributional semantics: "a word is characterized by the company it keeps" (John Firth 1957) <!-- .element: class="fragment" data-fragment-index="3" -->
* Discovers words which frequently occur together or in similar contexts (=topics)  <!-- .element: class="fragment" data-fragment-index="4" -->
* Infers how important each word is in each topic <!-- .element: class="fragment" data-fragment-index="5" -->
* Infers how important each topic is in each document <!-- .element: class="fragment" data-fragment-index="6" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
## Generative, inverted, iterative
<br/>
>"A topic model is a generative model for documents: it specifies a simple probabilistic procedure by which documents can be generated. To make a new document, one chooses a distribution over topics. Then, for each word in that document, one chooses a topic at random according to this distribution, and draws a word from that topic. Standard statistical techniques can be used to invert this process, inferring the set of topics that were responsible for generating a collection of documents."

<br/>
<p>(Steyvers and Griffiths, "Probabilistic Topic Modeling", 2006)</p>
</script>
</section>



<section data-markdown>
<script type="text/template">
## Inference problem: observed data
<p><img height="500" data-src="img/blei_topic-modeling_observed.png"></img></p>
<p>(David Blei, "Topic Models" lecture, 2009)</p>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Inferred, latent model
<p><img height="500" data-src="img/tm_blei.png"></img></p>
<p>(David Blei, "Probabilistic Topic Models", 2012)</p>
</script>
</section>



<section data-markdown>
<script type="text/template">
## Bayesian Statistics
>"The computational problem of inferring the hidden topic structure from the documents is the problem of computing the posterior distribution, the conditional distribution of the hidden variables given the documents."

<br/>
<p>(David Blei, "Probabilistic Topic Models", 2012)</p>
</script>
</section>


<section data-markdown>
<script type="text/template">
## Inference task
<br/>
###p(Z, φ, θ | w, α, β)
<br/>
<small>

* Compute the probability p of the latent variables... 
	* Z = assignments of each word in each document to a topic
	* φ (phi) = distribution over words (for each topic)
	* θ (theta) = distribution over topics (for each document)
* ...given our observed variables (input data) and parameters
	* w = the words in each document
	* α = parameter of the Dirichlet prior for topic per document
	* β = parameter of the Dirichlet prior for words per topic

</small>
</script>
</section>


<section data-markdown>
<script type="text/template">
## The starting point of LDA
* We have the documents with their words (e.g. as a word/document frequency matrix)  <!-- .element: class="fragment" data-fragment-index="1" -->
* We are looking for the word distributions per topic, the topic distributions per document, and the topic assignment of each word  <!-- .element: class="fragment" data-fragment-index="2" -->
* Both distributions are dependent on each other (if a topic changes, the topic distributions change)  <!-- .element: class="fragment" data-fragment-index="3" -->
* And both distributions need to fit with the original documents  <!-- .element: class="fragment" data-fragment-index="4" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
## The generative model behing LDA
* For each topic, there is a distribution over words <!-- .element: class="fragment" data-fragment-index="1" -->
* For each document, there is a distribution over topics <!-- .element: class="fragment" data-fragment-index="2" -->
* For each word in each document: <!-- .element: class="fragment" data-fragment-index="3" -->
    * We sample a topic from the topic distribution of that document.
    * We sample a word from the word distribution of that topic.
* (This can only work if we have the distributions; which we don't) <!-- .element: class="fragment" data-fragment-index="4" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## Random initialization
* For each document, we generate a random distribution over topics <!-- .element: class="fragment" data-fragment-index="1" -->
* For each topic, we generate a random distribution over words <!-- .element: class="fragment" data-fragment-index="2" -->
* For each word in each document:  <!-- .element: class="fragment" data-fragment-index="3" -->
    * Sample a topic from the topic distribution
    * Sample a word from the word distribution of that topic
* (Now we have a model; but we know it's most likely wrong (=low confidence) <!-- .element: class="fragment" data-fragment-index="4" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## Inference: iterative approximation
* Using the observed data and our (random/erroneous) model, we can improve the model <!-- .element: class="fragment" data-fragment-index="1" -->
* One among several methods: Gibbs sampling <!-- .element: class="fragment" data-fragment-index="2" -->
	* For one word in one document, remove the existing topic assignment <!-- .element: class="fragment" data-fragment-index="3" -->
    * Based on the model and the other words in the document, assign a new topic to the word (cooccurrence!) <!-- .element: class="fragment" data-fragment-index="4" -->
    * Update the overall model according to this assignment;  <!-- .element: class="fragment" data-fragment-index="5" -->
* Repeat until your time runs out or your evaluation task says it's ok to stop <!-- .element: class="fragment" data-fragment-index="6" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
## How it works exactly, clearly explained
<p><img height="600" data-src="img/sharris_more-explicit-in-step-two.jpg"></img></p>  <!-- .element: class="fragment" data-fragment-index="1" -->
</script>
</section>

<!--
<section data-markdown>
<script type="text/template">
## Topic modeling: geometric interpretation
<p><img height="500" data-src="img/steyvers_geometric-interpretation-of-the-topic-model.png"></img></p>
<p>(Steyvers and Griffiths, "Probabilistic Topic Modeling", 2006)</p>
</script>
</section>
-->


<section data-markdown>
<script type="text/template">
## Latent Dirichlet Allocation: plate notation
<p><img height="500" data-src="img/steyvers_topic-modeling-plate-notation.png"></img></p>
<p>(<https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>)</p>
</script>
</section>



<!--
<section data-markdown>
<script type="text/template">
## Latent Dirichlet Allocation: plate notation
<p><img height="400" data-src="img/wikipedia_topic-modeling-plate-notation_legend.png"></img></p>
<p>(<https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>)</p>
</script>
</section>
-->



<section data-markdown>
<script type="text/template">
## Latent Dirichlet Allocation: plate notation
<small>

* D = number of documents (given)
* T = number of topics (set by researcher)
* Nd = number of words in document d
* α (alpha): Dirichlet prior (hyperparameter: sparse / smooth distribution of topics)
* β (beta): Dirichlet prior (hyperparameter: sparse / smooth distribution of words)
* θ (theta): distribution over topics (for each document; latent variable)
* ϕ (phi): distribution over words (for each topic; latent variable) 
* z = assignment of words to topics (latent variable)
* w = words in a document (observed variable)

</small>
</script>
</section>



</section>
<section>


<section data-markdown>
<script type="text/template">
# 2. Interpreting Topic Models
</script>
</section>


<section data-markdown>
<script type="text/template">
## (a) Example analysis: French crime fiction
</script>
</section>

<section data-markdown>
<script type="text/template">
## Text collection: 840 French Novels
<img height="500" data-src="img/textsammlung.png"></img>
</script>
</section>



<section data-markdown>
<script type="text/template">
## Crime fiction (prototypical)
* Long, narrative, fictional prose (=novel) <!-- .element: class="fragment" data-fragment-index="1" -->
* Character inventory: investigators, criminals, suspects, witnesses, victims <!-- .element: class="fragment" data-fragment-index="2" -->
* Plot: violent crime, rational elucidation <!-- .element: class="fragment" data-fragment-index="3" -->
* Setting: urban space <!-- .element: class="fragment" data-fragment-index="4" -->
* => Hypotheses regarding possible topics <!-- .element: class="fragment" data-fragment-index="5" -->
</script>
</section>



<section data-markdown>
<script type="text/template">
## Topic and subgenre
<p><a href="img/2_topic10-wordle-comparison.png"><img height="400" data-src="img/2_topic10-wordle-comparison.png"></img></a></p>
<p><b>Topic 10: detective, inspector, police</b></p>
<p>Distinctive of crime fiction (content & statistics) (p &lt; α=0.01)</p> <!-- .element: class="fragment" data-fragment-index="1" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
##Topic and subgenre
<p><a href="img/2_topic49-wordle-comparison.png"><img height="400" data-src="img/2_topic49-wordle-comparison.png"></img></a></p>
<p><b>Topic 49: death, crime, to kill</b></p>
<p>Distinctive of crime fiction (content & statistics) (p &lt; α=0.01)</p>
</script>
</section>


<section data-markdown>
<script type="text/template">
##Topic and subgenre
<p><a href="img/2_topic47-wordle-comparison.png"><img height="400" data-src="img/2_topic47-wordle-comparison.png"></img></a></p>
<p><b>Topic 47: door, room, to open</b></p>
<p>Statistically distinctive (p &lt; α=0.01); but content-wise?</p> 
</script>
</section>


<section data-markdown>
<script type="text/template">
##Topic and subgenre
<p><a href="img/2_topic26-wordle-comparison.png"><img height="400" data-src="img/2_topic26-wordle-comparison.png"></img></a></p>
<p><b>Topic 26: beach, sand, sun</b></p>
<p>Distinctive of non-crime fiction (p &lt; α=0.001)</p>
</script>
</section>


<section data-markdown>
<script type="text/template">
## Topics over text segments
<p><a href="img/2_topic002_progression.png"><img height="400" data-src="img/2_topic002_progression.png"></img></a></p>
<p><b>Topic 2: judge, prison, lawyer/attorney</b></p>
<p>Statistically significant (crime fiction): (1,4), (4,5) etc.</p>
</script>
</section>


<section data-markdown>
<script type="text/template">
## Topics over text segments
<p><a href="img/2_topic033_progression.png"><img height="400" data-src="img/2_topic033_progression.png"></img></a></p>
<p><b>Topic 33: black, hair, eyes, wear, eye, face</b></p>
<p>Statistically significant: crime fiction all but (2,3);<br/> non-crime fiction (1,3), (2,5)</p>
</script>
</section>



<section data-markdown>
<script type="text/template">
## Overall results
* A large part of the topics is statistically distinctive: crime fiction (31/80) non-crime fiction (21/80) <!-- .element: class="fragment" data-fragment-index="1" -->
* Topics are not just themes, but also narrative motives, descriptive elements, character sets <!-- .element: class="fragment" data-fragment-index="2" -->
* Textual progression: only a few topics have significant trends <!-- .element: class="fragment" data-fragment-index="3" -->
* Overall: we can detect thematic trends in 840 novels without reading (all of) them! <!-- .element: class="fragment" data-fragment-index="4" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
## (b) Some more visualizations
</script>
</section>

<section data-markdown>
<script type="text/template">
## Topics and subgenres: topic 3
<img height="500" data-src="img/tI_by-subsubgenre-003.png"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Topics and authors: topic 3
<img height="500" data-src="img/tI_by-author-name-003.png"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Topics / subgenres heatmap
<img height="500" data-src="img/dist-heatmap_by-subsubgenre_policier-smallset.jpg"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## topic clustering
<img height="500" data-src="img/topic-clustering_cosine-weighted-50words.png"></img>
<p>(top 50 topics, cosine/weighted)</p>
</script>
</section>

<section data-markdown>
<script type="text/template">
## topic clustering (detail)
<img height="500" data-src="img/topic-clustering_cosine-weighted-50words_3-227.png"></img>
<p>(top 50 topics, cosine/weighted)</p>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Topics over decades
<img height="500" data-src="img/lineplot-3-227.png"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Topics and authors: clustering
<img height="500" data-src="img/item-clustering_author-name_cosine-weighted-mean-50topics.jpg"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## topic-work bimodal network
<img height="500" data-src="img/author-topic-map_full.jpg"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## topic-work bimodal network (detail)
<img height="500" data-src="img/author-topic-map_sub-neopolar.jpg"></img>
</script>
</section>


<section data-markdown>
<script type="text/template">
## topics over text progression
<img height="500" data-src="img/topics_fallend.png"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## topics over text progression
<img height="500" data-src="img/topics_ansteigend.png"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## topics by genre and text progression
<img height="500" data-src="img/textverlauf-untergattung.png"></img>
</script>
</section>

<section data-markdown>
<script type="text/template">
## PCA based on topic scores (subgenres)
<img height="500" data-src="img/topic-pca.png"></img>
</script>
</section>


</section>
<section>

<section data-markdown>
<script type="text/template">
# 3. Hands-on with Mallet
</script>
</section>


<section data-markdown>
<script type="text/template">
## (a) Fundamentals
</script>
</section>


<section data-markdown>
<script type="text/template">
## (2) Topic Modeling Workflow
<p><a href="img/topic-modeling-workflow_EN.png"><img height="500" data-src="img/topic-modeling-workflow_EN.png"></img></a></p>
<p>(Mallet and Python; see <a href="http://github.com/cligs/tmw">github.com/cligs/tmw</a>)</p>
</script>
</section>


<section data-markdown>
<script type="text/template">
## Parameters
* Preprocessing: length of the text segments, lemmatization, feature selection <!-- .element: class="fragment" data-fragment-index="1" -->
* Modelling: number of topics, number of iterations, distribution optimization  <!-- .element: class="fragment" data-fragment-index="2" -->
* Visualization: aggregation strategy, normalization, choice of visualization  <!-- .element: class="fragment" data-fragment-index="3" -->
</script>
</section>


<section data-markdown>
<script type="text/template">
## MALLET: the heart of topic modeling
* Preprocessing is done elsewhere (eg., with tmw)
* Postprocessing and visualisation is done elsewhere (eg., with tmw)
* Mallet imports texts and builds the topic model (and does it really well)
* Attention! Mallet is operated from the command line
* Find help: <http://mallet.cs.umass.edu/topics.php>
</script>
</section>


<section data-markdown>
<script type="text/template">
## (b) Getting started
</script>
</section>


<section data-markdown>
<script type="text/template">
## Some useful links
* This section of the slides: <br/>https://christofs.github.io/topic-modeling-edinburgh/#/4/6 
* The dataset (Hong Kong Universities' Press Releases): <br/> <https://dl.dropboxusercontent.com/u/1786135/press.zip>
* The MALLET Topic Modeling page: <br/>http://mallet.cs.umass.edu/topics.php
</script>
</section>



<section data-markdown>
<script type="text/template">
## Mallet: prerequisites
* Mallet 2.0.8 is installed
    * in `/home/[USER]/Programs/mallet-2.0.8` on Linux
    * or in `C:\Programs\mallet-2.0.8` on Windows
* A working directory (here: on the Desktop), called "press/"
* A simple text editor: Geany, Notepadd++, TextEdit 
* Open text editor and create new file called "run-mallet.txt"
</script>
</section>


<section data-markdown>
<script type="text/template">
## Testing Mallet
(using the Linux Terminal; from "/home/[USER]")
```
cd ~/Programs/mallet-2.0.8/
./bin/mallet
```
<br/>
(using the Windows CMD; from "C:")
```
cd C:\Programs\mallet-2.0.8
.\bin\mallet
```
(List of commands should show up.)
</script>
</section>


<section data-markdown>
<script type="text/template">
## Folder structure
* /press/
   * 1-texts/
   * 2-mallet/
   * 3-derived
   * metadata.csv
   * stopwords.txt
</script>
</section>

<section data-markdown>
<script type="text/template">
## Calling Mallet: two steps
* "import"
    * Converts the input text files into the Mallet corpus format
    * Writes a binary file
* "train-topics"
    * Performs the actual modeling on the corpus
    * Writes several tabular files
</script>
</section>


<section data-markdown>
<script type="text/template">
## (b) The details
</script>
</section>


<section data-markdown>
<script type="text/template">
## (1) Importing texts into Mallet
* tell the computer to use Mallet
* tell Mallet to import texts
* ...where the text files are
* ...where to save the corpus to
* (optionally: ...how to tokenize the texts)
* (optionally: ...whether to use stopwords)
</script>
</section>

<section data-markdown>
<script type="text/template">
## Importing texts into Mallet (Linux)
(using the Terminal; from "mallet-2.0.8" folder)
```
./bin/mallet import-dir 
--input ~/Desktop/press/1-texts/
--output ~/Desktop/press/2-mallet/press.mallet
--keep-sequence
--remove-stopwords TRUE
--stoplist-file ~/Desktop/press/stopwords.txt
```
</script>
</section>


<section data-markdown>
<script type="text/template">
## Importing texts into Mallet (Windows)
(using the `cmd` Terminal; from `C:\Programs\mallet-2.0.8\`)
```
bin\mallet import-dir 
--input C:\Users\[USER]\Desktop\press\1-texts 
--output C:\Users\[USER]\Desktop\press\2-mallet\press.mallet 
--keep-sequence 
--remove-stopwords TRUE 
--stoplist-file C:\Users\[USER]\Desktop\press\stopwords.txt
```
</script>
</section>



<section data-markdown>
<script type="text/template">
## (2) Build the topic model
* tell the computer to run Mallet
* tell Mallet to do some modeling (`train-topics`)
* ...where the corpus file is (`--input`)
* ...how many topics (`--num-topics`)
* ...how many iterations (`--num-iterations`)
* ...how many words to display (`--num-topic-words`)
* ...path for words-by-topics (`--output-topic-keys`)
* ...path for topics-per-doc (`--output-doc-topics`)
</script>
</section>

<section data-markdown>
<script type="text/template">
## (2) Build the topic model (Linux)
<p>(from "mallet-2.0.8/")</p>
```
./bin/mallet train-topics  
--input ~/Desktop/press/2-mallet/press.mallet 
--num-topics 20 
--num-iterations 200 
--num-top-words 10 
--output-topic-keys ~/Desktop/press/2-mallet/topics-words.txt 
--output-doc-topics ~/Desktop/press/2-mallet/docs-topics.txt 
--doc-topics-max 20
```
</script>
</section>

<section data-markdown>
<script type="text/template">
## (2) Build the topic model (Windows)
<p>(from `C:\Programs\mallet-2.0.8\`)</p>
```
bin\mallet train-topics 
--input C:\\Users\[USER]\Desktop\press\2-mallet\press.mallet 
--num-topics 20 
--num-iterations 200 
--num-top-words 10 
--output-topic-keys C:\\Users\[USER]\Desktop\press\2-mallet\topics-words.txt 
--output-doc-topics C:\\Users\[USER]\Desktop\press\2-mallet\docs-topics.txt 
--doc-topics-max 20
```
</script>
</section>

<section data-markdown>
<script type="text/template">
## Inspecting the results
* Open the `topics-words.txt` file with text editor (or Calc/Excel)
* Open the `docs-topics.txt` file with text editor (or Calc/Excel)
</script>
</section>


<section data-markdown>
<script type="text/template">
## (d) Practice
</script>
</section>

<section data-markdown>
<script type="text/template">
## Preparation
* Open "run-mallet.txt" in text editor
* Find the Mallet installation folder on your computer and copy the path into your text file
* Find the "press" folder on your computer and copy the path into your text file
* Copy the generic `import` and `train-topic` commands from the presentation into your text file
* Now you're all set!
</script>
</section>

<section data-markdown>
<script type="text/template">
## Exercise 1: Import and model
* Build the `import` and `train-topic` commands for your system;
* remember to remove all line breaks in the command
* Use the `import` comand and make sure the right `press.mallet` corpus appears
* Use the `train-topics` command and make sure the output files appear
* Inspect the results; do you get nice topics?
</script>
</section>


<section data-markdown>
<script type="text/template">
## Exercise 2: Adapt the commands
* At your choice, do *one* of the following 
    * Deactivate or modify the stopword list (on import)
    * Use the `1-prepd` files (on import)
    * Use a different number of topics
    * Perform fewer or more iterations
    * Display more or less topic words
    * Add the parameter `--optimize-interval 10` to your modeling command
* Make sure the files are written to a new folder (e.g. "2-mallet_v2/")
* Inspect the results and write down any changes you notice
</script>
</section>

</section>
<section>

<section data-markdown>
<script type="text/template">
# Discussion
</script>
</section>

<section data-markdown>
<script type="text/template">
## Summary of what we have covered
* An idea of what a topic model consists of <!-- .element: class="fragment" data-fragment-index="1" -->
* An intuition of how topic models are inferred <!-- .element: class="fragment" data-fragment-index="2" -->
* Some avenues for interpreting and visualizing topic models <!-- .element: class="fragment" data-fragment-index="3" -->
* The overall workflow required for topic modeling <!-- .element: class="fragment" data-fragment-index="4" -->
* How to use MALLET for topic modeling <!-- .element: class="fragment" data-fragment-index="5" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## A few things not covered here
* Preprocessing, Postprocessing, Visualization <!-- .element: class="fragment" data-fragment-index="1" -->
* Implementation details of Gibbs Sampling <!-- .element: class="fragment" data-fragment-index="2" -->
* Precursors of LDA: LSA, pLSA, NNMF, etc. <!-- .element: class="fragment" data-fragment-index="3" -->
* Variants of LDA: hierarchical, labeled, dynamic, etc. <!-- .element: class="fragment" data-fragment-index="4" -->
* Evaluation strategies: human evaluation, external, internal  <!-- .element: class="fragment" data-fragment-index="5" -->
</script>
</section>

<section data-markdown>
<script type="text/template">
## Your questions and projects
* What kind of projects / text collections do you have?
* What kind of research questions do you have?
* What do you think topic modeling could tell you?
* ...
</script>
</section>


<section data-markdown>
<script type="text/template">
## Further Reading: Theory and Tutorials
<small>

**Introductory articles**
* Blei, David M. (2012). "Probabilistic topic models". In: _Communications of the ACM_, 55(4): 77–84. <http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf>
* Steyvers, M. and Griffiths, T. (2006). "Probabilistic Topic Models". In: Landauer, T. et al. (eds), _Latent Semantic Analysis: A Road to Meaning_. Laurence Erlbaum.

**Tutorials**
* Weingart, Scott (2012). "Topic Modeling for Humanists: A Guided Tour". In: _The Scottbot Irregular_. <http://www.scottbot.net/HIAL/?p=19113>
* Graham, Shawn, Scott Weingart and Ian Milligan (2012). "Getting Started with Topic Modeling and MALLET". _The Programming Historian_. <http://programminghistorian.org/lessons/topic-modeling-and-mallet>
* Riddell, Allen. (2014). "TAToM: Text Analysis with Topic Modeling for Humanities Scholars". In: _DARIAH-DE Schulungsmaterialien_. <https://de.dariah.eu/tatom/>

**Video lectures**
* Jordan Boyd-Graber, "Topic Models", YouTube.com, 2015. <https://www.youtube.com/watch?v=yK7nN3FcgUs>
* David Blei, "Topic Models", Videolectures.net, 2012, <http://videolectures.net/mlss09uk_blei_tm/>

</small>
</script>
</section>

<section data-markdown>
<script type="text/template">
## Further Reading: Applications
<small>

* Blevins, Cameron (2010). "Topic Modeling Martha Ballard’s Diary". In: _Historying_. <http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/>
* Jockers, Matt (2013). _Macroanalysis - Digital Methods and Literary History_. Champaign, IL: University of Illinois Press.
* Rhody, Lisa (2012). "Topic Modeling and Figurative Language". In: _Journal of Digital Humanities_, 2(1). <http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/>
* Schöch, Christof (2017, to appear). "Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama". In: _Digital Humanities Quarterly_. <https://zenodo.org/record/166356>
* Underwood, Ted and Andrew Goldstone (2012)." "What can topic models of PMLA teach us about the history of literary scholarship?" In: _The Stone and the Shell_. <http://tedunderwood.com/2012/12/14/what-can-topic-models-of-pmla-teach-us-about-the-history-of-literary-scholarship/>

</small>
</script>
</section>

</section>


<section data-markdown>
<script type="text/template">
<br/>
<br/>
<br/>
## Thank you!
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<hr/>
<p>Christof Schöch, 2017</p>
<p><a href="https://christofs.github.io/">christofs.github.io</a></p>
<p><a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a><br/></p>
<hr/>
<br/>
<br/>
</script>
</section>



<!-- DON'T TOUCH UNLESS YOU KNOW WHAT YOU'RE DOING :-) -->
</div>
<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>
<script>
// Full list of configuration options available at:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    transition: 'slide', // none/fade/slide/convex/concave/zoom
    // Optional reveal.js plugins
    dependencies: [
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: 'plugin/zoom-js/zoom.js', async: true },
        { src: 'plugin/notes/notes.js', async: true }
        ]
    });
</script>
</body>
</html>
